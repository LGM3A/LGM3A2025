
<!-- saved from url=(0046)https://mm2022-apccpa-workshop.github.io/#call -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- SITE TITTLE -->
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Workshop LGM3A 2025</title>
    
    <!-- PLUGINS CSS STYLE -->
    <!-- link rel="icon" href="https://2022.acmmm.org/wp-content/themes/acmmultimedia/assets/images/favicon-acm.png" -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/jquery-ui.min.css" rel="stylesheet">
    <!-- Bootstrap -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/font-awesome.min.css" rel="stylesheet">
    <!-- Owl Carousel -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/slick.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/slick-theme.css" rel="stylesheet">
    <!-- Fancy Box -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/jquery.fancybox.pack.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/nice-select.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/bootstrap-slider.min.css" rel="stylesheet">
    <!-- CUSTOM CSS -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/style.css" rel="stylesheet">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
   <style type="text/css">
    .left,
    .right {
      float: left;
      width: 50%;
      padding-left: 80px;
    }
    .left { padding-left: 250px; }
  </style> 
    </head>
    
    <body class="body-wrapper" data-new-gr-c-s-check-loaded="14.997.0" data-gr-ext-installed="" style="background:hsl(108, 41%, 73%);">
        <section>
          <div class="container">
            <div class="row">
              <div class="col-md-12">
                <nav class="navbar navbar-expand-lg  navigation"> <a class="navbar-brand" href="https://www.acmmm2023.org/"> <img src="./LGM3A_Workshop_ACMMM2025_files/logo-acm-2025.png" alt="" width="100px"> </a>
                  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
                  <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ml-auto main-nav">
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#overview"><font color="white"><strong>Home</strong></font></a></li>
					  
					  <li class="nav-item"> <a class="nav-link " href="https://lgm3a.github.io/LGM3A2025/#announcements"><font color="white"><strong>News</strong></font></a></li>

                      <li class="nav-item"> <a class="nav-link " href="https://lgm3a.github.io/LGM3A2025/#call"><font color="white"><strong>Call for papers</strong></font></a></li>
      
					  <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#submission"><font color="white"><strong>Submission</strong></font></a></li>
	  
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#organizers"><font color="white"><strong>Organizers</strong></font></a></li>

                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#speakers"><font color="white"><strong>Speakers</strong></font></a></li>
    
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#schedule"><font color="white"><strong>Schedule</strong></font></a></li>
                    </ul>
                    
                  </div>
                </nav>
              </div>
            </div>
          </div>
        </section>
    
    
    <!--==========================================
    =            Overview Section            =
    ===========================================-->
    
    <section class="popular-deals section bg-white"> 
      <!-- Container Start -->
      <div class="container">
        <div class="row">
          <div class="col-md-12"> 
            <!-- Section title -->
            <div class="section-title">
              <h1>Large Generative Models Meet Multimodal Applications (LGM3A)</h1>
			  <br>
			  <h4>Workshop at ACM Multimedia 2024</h4>
			  <br><br>
              <h2 id="overivew">Scope and Topics</h2>
            </div>
        This workshop aims to explore the potential of large generative models to revolutionize the way we interact with multimodal information. 
        A Large Language Model (LLM) represents a sophisticated form of artificial intelligence engineered to comprehend and produce natural language text, exemplified by technologies such as GPT, LLaMA, Flan-T5, ChatGLM, and Qwen, etc. 
        These models undergo training on extensive text datasets, exhibiting commendable attributes including robust language generation, zero-shot transfer capabilities, and In-Context Learning (ICL). 
        With the surge in multimodal content—encompassing images, videos, audio, and 3D models—over the recent period, Large MultiModal Models (LMMs) have seen significant enhancements. 
        These improvements enable the augmentation of conventional LLMs to accommodate multimodal inputs or outputs, as seen in BLIP, Flamingo, KOSMOS, LLaVA, Gemini, GPT-4, etc. 
        Concurrently, certain research initiatives have delved into generating specific modalities, with Kosmos2 and MiniGPT-5 focusing on image generation, and SpeechGPT on speech production. 
        There are also endeavors to integrate LLMs with external tools to achieve a near 'any-to-any' multimodal comprehension and generation capacity, illustrated by projects like Visual-ChatGPT, ViperGPT, MMREACT, HuggingGPT, and AudioGPT. 
        Collectively, these models, spanning not only text and image generation but also other modalities, are referred to as large generative models. 
        This workshop will provide an opportunity for researchers, practitioners, and industry professionals to explore the latest trends and best practices in the field of multimodal applications of large generative models. 
        We also remark that the submissions are not limited to the use of such models. The workshop will also focus on exploring the challenges and opportunities of integrating large language models with other AI technologies such as computer vision and speech recognition. 
        Additionally, the workshop will provide a platform for participants to present their research, share their experiences, and discuss potential collaborations. 
        </div>
        </div>
      </div>
    </section>
    

    
    <!--===========================================
    =            Dates Section            =
    ============================================-->

    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="announcements">News</h2>
            </div>
			<ul style="margin-left: 40px">
      <!-- <li>28/8/2023 - Workshop schedule is announced. </li>
      <li>8/8/2023 - Workshop papers notification is announced. </li>
      <li>19/7/2023 - Workshop papers submission is delayed. </li>
      <li>27/4/2023 - Important dates are updated. </li> -->
      <li>7/5/2023 - CFP is released. </li> 
	  <li>7/5/2023 - Workshop homepage is now available. </li>
			</ul>

          </div>
        </div>
      </div>
    </section>

	
	<!--==========================================
    =            Call for Papers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="call">Call for Papers</h2>
            </div>
           This workshop intends to 1) provide a platform for researchers to present their latest works and receive feedback from experts in the field, 
		   2) foster discussions on current challenges and opportunities in multimodal analysis and application, 
		   3) identify emerging trends and opportunities in the field, and 
		   4) explore their potential impact on future research and development. Potential topics include, but are not limited to:
			<ul style="margin-left: 40px">

      <li>Multimodal data augmentation</li>
      <li>Multimodal data analysis and understanding</li>
      <li>Multimodal question answering</li>
      <li>Multimodal generation</li>
      <li>Multimodal retrieval augmentation</li>
      <li>Multimodal recommendation </li>
      <li>Multimodal summarization and text generation</li>
      <li>Multimodal agents</li>
      <li>Multimodal prompting</li>
      <li>Multimodal continual learning</li>
      <li>Multimodal fusion and integration of information</li>
      <li>Multimodal applications/pipelines</li>
      <li>Multimodal systems management and indexing</li>
      <li>Multimodal mobile/lightweight deployment</li>
			</ul>

      Important dates: 
			<ul style="margin-left: 40px">

        <li>Workshop Papers Submission: <strong>TBD</strong></li>
	    <li>Workshop Papers Notification: <strong>TBD</strong></li>
        <li>Camera-ready Submission: <strong>TBD</strong></li>
		<li>Conference dates: 27 - 31 October 2025</li>
        </ul>
		Please note: The submission deadline is at 11:59 p.m. of the stated deadline date&nbsp;<a href="https://time.is/Anywhere_on_Earth"><font color="red"><u>Anywhere on Earth</u></font></a>.</p>

          </div>
        </div>
      </div>
    </section>

    <!--==========================================
    =            Papers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="submission">Submission</h2>
            </div>
            <li><strong>Submission Guidelines</strong>:</li>
			Submitted papers (.pdf format) must be the same format & template as the main conference. 
      The submition format The manuscript’s length is limited to one of the two options: a) 4 pages plus 1-page reference; or b) 8 pages plus up to 2-page reference.
      All papers will be peer-reviewed by experts in the field. Acceptance will be based on relevance to the workshop, scientific novelty, and technical quality. 
      <!--The workshop papers will be published in the ACM Digital Library.--> 
			<li><strong>Submission Site</strong>:
			<a href="https://easychair.org/conferences/?conf=lgm3a"><font color=#5672f9><u>https://easychair.org/conferences/?conf=lgm3a</u></font></a></li>
          </div>
        </div>
      </div>
    </section>	



	
    <!--==========================================
    =            Organizers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="organizers">Organizers</h2>
            </div>
			<ul style="margin-left: 40px">
			<!-- <li><a href="https://zhengwang125.github.io/"><font color=#5672f9><u>Zheng Wang</u></font></a> (Huawei Singapore Research Center, Singapore)</li>
			<li><a href="https://personal.ntu.edu.sg/c.long/index.html"><font color=#5672f9><u>Cheng Long</u></font></a> (Nanyang Technological University, Singapore)</li>
			<li><font color=#5672f9><u>Shihao Xu</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><font color=#5672f9><u>Bingzheng Gan</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><font color=#5672f9><u>Wei Shi</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><a href="https://scholar.google.com/citations?user=aJmTPaoAAAAJ&hl=en"><font color=#5672f9><u>Zhao Cao</u></font></a> (Huawei Technologies Co., Ltd, China)</li>
			<li><a href="https://www.chuatatseng.com/"><font color=#5672f9><u>Tat-Seng Chua</u></font></a> (National University of Singapore, Singapore)</li> -->

      <li><font color=#5672f9><u>Shihao Xu</u></font> (Huawei Singapore Research Center, Singapore)</li>
      <li><font color=#5672f9><u>Yiyang Luo</u></font> (Huawei Singapore Research Center, Singapore)</li>
      <li><font color=#5672f9><u>Justin Dauwels</u></font></a> (Delft University of Technology)</li>
      <li><font color=#5672f9><u>Andy Khong</u></font></a> (Nanyang Technological University, Singapore)</li>
      <li><font color=#5672f9><u>Zheng Wang</u></font></a> (Huawei Singapore Research Center, Singapore)</li>
			<li><font color=#5672f9><u>Qianqian Chen</u></font> (Huawei Singapore Research Center, Singapore)</li>
      <li><font color=#5672f9><u>Chen Cai</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><font color=#5672f9><u>Wei Shi</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><a href="https://www.chuatatseng.com/"><font color=#5672f9><u>Tat-Seng Chua</u></font></a> (National University of Singapore, Singapore)</li>
			</ul>

    <!--==========================================
    =            Speakers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="speakers">Speakers</h2>
            </div>
            
	    <h3><b>Keynote 1</b></h3> 
                    
            TBD
            <!-- <img src="https://liuziwei7.github.io/homepage_files/me.png" class="media-left pull-left" width="170" alt="" margin="10" hspace="10" vspace="5"> 
            </a> 

            <a href="https://liuziwei7.github.io/"><font color=#5672f9><u>Prof. Ziwei Liu</u></font></a> is a Nanyang Assistant Professor (2020-) at College of Computing and Data Science in Nanyang Technological University, with MMLab@NTU. Previously, he was a research fellow (2018-2020) in CUHK with Prof. Dahua Lin and a post-doc researcher (2017-2018) in UC Berkeley with Prof. Stella Yu. His research interests include computer vision, machine learning and computer graphics. Ziwei received his Ph.D. (2013-2017) from CUHK, Multimedia Lab, advised by Prof. Xiaoou Tang and Prof. Xiaogang Wang. He is fortunate to have internships at Microsoft Research and Google Research. Ziwei is the recipient of MIT Technology Review Innovators under 35 Asia Pacific, ICBS Frontiers of Science Award, CVPR Best Paper Award Candidate and WAIC Yunfan Award. His works have been transferred to products, including Microsoft Pix, SenseGo and Google Clips.
            </p>

            <b>Talk Title:</b> Multi-Modal Generative AI with Foundation Models
            </br>
            <b>Abstract:</b> Generating photorealistic and controllable visual contents has been a long-pursuing goal of artificial intelligence (AI), with extensive real-world applications. It is also at the core of embodied intelligence. In this talk, I will discuss our work in AI-driven visual context generation of humans, objects and scenes, with an emphasis on combining the power of neural rendering with large multimodal foundation models. Our generative AI framework has shown its effectiveness and generalizability on a wide range of tasks. -->

	    <br></br>
		  
            <h3><b>Keynote 2</b></h3> 
                    
            TBD
            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->
            
            <!-- <img src="https://cde.nus.edu.sg/ece/wp-content/uploads/sites/3/2021/05/zhengshou_stanford_photo.jpg" class="media-left pull-left" width="170" height="180" alt="" margin="10" hspace="10" vspace="5"> 
            </a> 

            <a href="https://sites.google.com/view/showlab"><font color=#5672f9><u>Prof. Mike Zheng Shou</u></font></a> is a tenure-track Assistant Professor at National University of Singapore and a former Research Scientist at Facebook AI in the Bay Area. He holds a PhD degree from Columbia University in the City of New York, where he worked with Prof. Shih-Fu Chang. He was awarded the Wei Family Private Foundation Fellowship. He received the best paper finalist at CVPR'22 and the best student paper nomination at CVPR'17. His team won 1st place in multiple international challenges including ActivityNet 2017, EPIC-Kitchens 2022, Ego4D 2022 & 2023. He is a Fellow of the National Research Foundation (NRF) Singapore and has been named on the Forbes 30 Under 30 Asia list. 
            
            </p>

            <b>Talk Title:</b> Multimodal Video Understanding and Generation
            </br>
            <b>Abstract:</b> Exciting progress has been made in multimodal video intelligence, including both understanding and generation, these two pillars in video. Despite being promising, several key challenges still remain. In this talk, I will introduce our attempts to address some of them. (1) For understanding, I will share All-in-one, which employs one single unified network for efficient video-language modeling, and EgoVLP, which is the first video-language pre-trained model for egocentric video. (2) For generation, I will introduce our study of efficient video diffusion models (i.e., Tune-A-Video, 4K GitHub stars). (3) Finally, I would like to discuss our recent exploration, Show-o, one single LLM that unifies multimodal understanding and generation. -->
            <br></br>

    <!--===========================================
    =            Workshop Arrangement            =
    ============================================-->
    
    <section class="popular-deals section bg-white">
      <div class="container">
        <div class="row">
          <div class="col-md-12">
            <div class="section-title">
              <h2 id="schedule">Workshop Schedule</h2>
            </div>
            <p align="center">
                <!-- <img src="./res/schedule.png" width="90%"> -->
                <!-- <big><b>TBD</b></big> -->
                <!-- <font color="Blue"><i><strong>TBA </strong></i></font> -->
                <li>On-site venue: TBD</li>
                <li>Date & Time: TBD</li>
                <!-- <li>Zoom link: <a href=""><font color=#5672f9><u>TBD</u></font></a></li> -->
                <li>Zoom link: <a><font color=#5672f9><u>TBD</u></font></a></li>
                  <br/>
                  <style type="text/css">
                    #myta td{
                      padding-right: 8px;
                    }
                  </style>
                  <table border="1" id="myta">

                    <tr> <td><strong>Time (GMT+11) </strong></td> <td>	<strong>Title</strong>	</td></tr>
                    <!-- <tr> <td>13:00 - 13:10</td> <td>	Welcome Message from the Chairs	</td></tr>
                    <tr> <td>13:10 - 13:50</td> <td>	Keynote 1: Multi-Modal Generative AI with Foundation Models </td></tr>
                    <tr> <td>13:50 - 14:30</td> <td>	Keynote 2: Multimodal Video Understanding and Generation </td></tr>
                    <tr> <td>14:30 - 14:40</td> <td>	Break </td></tr>
                    <tr> <td>14:40 - 15:00</td> <td>	Presentation 1: Geo-LLaVA: A Large Multi-Modal Model for Solving Geometry Math Problems with Meta In-Context Learning </td></tr>
                    <tr> <td>15:00 - 15:20</td> <td>	Presentation 2: Leveraging the Syntactic Structure of the Text Prompt to Enhance Object-Attribute Binding in Image Generation </td></tr>
                    <tr> <td>15:20 - 15:40</td> <td>	Presentation 3: SynthDoc: Bilingual Documents Synthesis for Visual Document Understanding </td></tr>
                    <tr> <td>15:40 - 16:00</td> <td>	Presentation 4: Multimodal Understanding: Investigating the Capabilities of Large Multimodel Models for Object Detection in XR Applications </td></tr>
                    <tr> <td>16:00 - 16:20</td> <td>	Presentation 5: A Method for Efficient Structured Data Generation with Large Language Models </td></tr>
                    <tr> <td>16:20 - 16:30</td> <td>	Workshop Closing </td></tr> -->
                    </table>
<br>
<br>
                    <!--
                    <table border="1">
                    <tbody><tr> <td><strong>Time (October 14, Local Portugal Time)</strong></td> <td>	<strong>Paper Title</strong>	</td></tr>
                    <tr> <td>14:00 PM - 14:15 PM</td> <td>	IPDAE: Improved Patch-Based Deep Autoencoder for Lossy Point Cloud Geometry Compression	</td></tr>
                    <tr> <td>14:15 PM - 14:30 PM</td> <td>	GRASP-Net: Geometric Residual Analysis and Synthesis for Point Cloud Compression	</td></tr>
                    <tr> <td>14:30 PM - 14:45 PM</td> <td>	Wiener Filter-Based Point Cloud Adaptive Denoising for Video-based Point Cloud Compression	</td></tr>
                    <tr> <td>14:45 PM - 15:00 PM</td> <td>	OpenPointCloud-V2: A Deep Learning Based Open-Source Algorithm Library of Point Cloud Processing	</td></tr>

                    <tr> <td>15:00 PM - 15:15 PM</td> <td>	End-to-End Point Cloud Geometry Compression and Analysis with Sparse Tensor	</td></tr>
                    <tr> <td>15:15 PM - 15:30 PM</td> <td>	Transformer and Upsampling-Based Point Cloud Compression	</td></tr>
                    <tr> <td>15:30 PM - 15:45 PM</td> <td>	Quality Evaluation of Machine Learning-based Point Cloud Coding Solutions	</td></tr>
                    <tr> <td>15:45 PM - 16:00 PM</td> <td>	View-Adaptive Streaming of Point Cloud Scenes through Combined Decomposition and Video-based Coding	</td></tr>
                    </tbody></table> -->
				
            </p>
          </div>
        </div>
      </div>
    </section>


    
    <!--============================
    =            Footer            =
    =============================-->
    
    <!-- Footer Bottom -->
    <!-- footer class="footer-bottom"> 
      <!-- Container Start -->
      <div class="container">
        <div class="row">
          <div class="col-sm-12 col-12"> 
            <!-- Copyright -->
            <div class="copyright">
              <p class="text-center">
                For any questions, please email to <a href="mailto:LGM3A2024@gmail.com"><font color=#5672f9><u>LGM3A2024@gmail.com</u></a>
              </p>
                <br class="clear">
            </div>
          </div>
        </div>
      </div>
      <!-- Container End --> 
    </footer>
    
    <!-- JAVASCRIPTS --> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery-ui.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/tether.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.raty-fa.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/popper.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/bootstrap.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/bootstrap-slider.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/slick.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.nice-select.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.fancybox.pack.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/SmoothScroll.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/scripts.js.download"></script -->
    
    
    

</body></html>