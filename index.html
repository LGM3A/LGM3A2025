<!-- saved from url=(0046)https://mm2022-apccpa-workshop.github.io/#call -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- SITE TITTLE -->
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Workshop LGM3A 2025</title>
    
    <!-- PLUGINS CSS STYLE -->
    <!-- link rel="icon" href="https://2022.acmmm.org/wp-content/themes/acmmultimedia/assets/images/favicon-acm.png" -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/jquery-ui.min.css" rel="stylesheet">
    <!-- Bootstrap -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/font-awesome.min.css" rel="stylesheet">
    <!-- Owl Carousel -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/slick.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/slick-theme.css" rel="stylesheet">
    <!-- Fancy Box -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/jquery.fancybox.pack.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/nice-select.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/bootstrap-slider.min.css" rel="stylesheet">
    <!-- CUSTOM CSS -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/style.css" rel="stylesheet">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
   <style type="text/css">
    .left,
    .right {
      float: left;
      width: 50%;
      padding-left: 80px;
    }
    .left { padding-left: 250px; }
    .speaker-photo {
      margin: 10px;
      float: left;
    }
    .speaker-block::after {
      content: "";
      display: table;
      clear: both;
    }
  </style> 
    </head>
    
    <body class="body-wrapper" data-new-gr-c-s-check-loaded="14.997.0" data-gr-ext-installed="" style="background:hsl(108, 41%, 73%);">
        <section>
          <div class="container">
            <div class="row">
              <div class="col-md-12">
                <nav class="navbar navbar-expand-lg  navigation"> <a class="navbar-brand" href="https://www.acmmm2025.org/"> <img src="./LGM3A_Workshop_ACMMM2025_files/logo-acm-2025.png" alt="" width="100px"> </a>
                  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
                  <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ml-auto main-nav">
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#overview"><font color="white"><strong>Home</strong></font></a></li>
					  
					  <li class="nav-item"> <a class="nav-link " href="https://lgm3a.github.io/LGM3A2025/#announcements"><font color="white"><strong>News</strong></font></a></li>

                      <li class="nav-item"> <a class="nav-link " href="https://lgm3a.github.io/LGM3A2025/#call"><font color="white"><strong>Call for papers</strong></font></a></li>
      
					  <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#submission"><font color="white"><strong>Submission</strong></font></a></li>
	  
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#organizers"><font color="white"><strong>Organizers</strong></font></a></li>

                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#speakers"><font color="white"><strong>Speakers</strong></font></a></li>
    
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2025/#schedule"><font color="white"><strong>Schedule</strong></font></a></li>
                    </ul>
                    
                  </div>
                </nav>
              </div>
            </div>
          </div>
        </section>
    
    
    <!--==========================================
    =            Overview Section            =
    ===========================================-->
    
    <section class="popular-deals section bg-white"> 
      <!-- Container Start -->
      <div class="container">
        <div class="row">
          <div class="col-md-12"> 
            <!-- Section title -->
            <div class="section-title">
              <h1>Large Generative Models Meet Multimodal Applications (LGM3A)</h1>
			  <br>
			  <h4>Workshop at ACM Multimedia 2025</h4>
			  <br><br>
              <h2 id="overivew">Scope and Topics</h2>
            </div>
        This workshop aims to explore the potential of large generative models to revolutionize the way we interact with multimodal information. 
        A Large Language Model (LLM) represents a sophisticated form of artificial intelligence engineered to comprehend and produce natural language text, exemplified by technologies such as GPT, LLaMA, Flan-T5, ChatGLM, and Qwen, etc. 
        These models undergo training on extensive text datasets, exhibiting commendable attributes including robust language generation, zero-shot transfer capabilities, and In-Context Learning (ICL). 
        With the surge in multimodal content—encompassing images, videos, audio, and 3D models—over the recent period, Large MultiModal Models (LMMs) have seen significant enhancements. 
        These improvements enable the augmentation of conventional LLMs to accommodate multimodal inputs or outputs, as seen in BLIP, Flamingo, KOSMOS, LLaVA, Gemini, GPT-4, etc. 
        Concurrently, certain research initiatives have delved into generating specific modalities, with Kosmos2 and MiniGPT-5 focusing on image generation, and SpeechGPT on speech production. 
        There are also endeavors to integrate LLMs with external tools to achieve a near 'any-to-any' multimodal comprehension and generation capacity, illustrated by projects like Visual-ChatGPT, ViperGPT, MMREACT, HuggingGPT, and AudioGPT. 
        Collectively, these models, spanning not only text and image generation but also other modalities, are referred to as large generative models. 
        This workshop will provide an opportunity for researchers, practitioners, and industry professionals to explore the latest trends and best practices in the field of multimodal applications of large generative models. 
        We also remark that the submissions are not limited to the use of such models. The workshop will also focus on exploring the challenges and opportunities of integrating large language models with other AI technologies such as computer vision and speech recognition. 
        Additionally, the workshop will provide a platform for participants to present their research, share their experiences, and discuss potential collaborations. 
        </div>
        </div>
      </div>
    </section>
    

    
    <!--===========================================
    =            Dates Section            =
    ============================================-->

    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="announcements">News</h2>
            </div>
			<ul style="margin-left: 40px">
      <!-- <li>28/8/2023 - Workshop schedule is announced. </li>
      <li>8/8/2023 - Workshop papers notification is announced. </li>
      <li>19/7/2023 - Workshop papers submission is delayed. </li>
      <li>27/4/2023 - Important dates are updated. </li> -->
      <li>28 March 2025 - Call For Paper is released. </li> 
	  <li>28 March 2025 - Workshop homepage is now available. </li>
			</ul>

          </div>
        </div>
      </div>
    </section>

	
	<!--==========================================
    =            Call for Papers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="call">Call for Papers</h2>
            </div>
           This workshop intends to 1) provide a platform for researchers to present their latest works and receive feedback from experts in the field, 
		   2) foster discussions on current challenges and opportunities in multimodal analysis and application, 
		   3) identify emerging trends and opportunities in the field, and 
		   4) explore their potential impact on future research and development. Potential topics include, but are not limited to:
			<ul style="margin-left: 40px">

      <li>Multimodal data augmentation</li>
      <li>Multimodal data analysis and understanding</li>
      <li>Multimodal question answering</li>
      <li>Multimodal generation</li>
      <li>Multimodal retrieval augmentation</li>
      <li>Multimodal recommendation </li>
      <li>Multimodal summarization and text generation</li>
      <li>Multimodal agents</li>
      <li>Multimodal prompting</li>
      <li>Multimodal continual learning</li>
      <li>Multimodal fusion and integration of information</li>
      <li>Multimodal applications/pipelines</li>
      <li>Multimodal systems management and indexing</li>
      <li>Multimodal mobile/lightweight deployment</li>
			</ul>

      Important dates: 
			<ul style="margin-left: 40px">

        <li>Workshop Papers Submission: <strong>11 July 2025</strong></li>
	    <li>Workshop Papers Notification: <strong>01 August 2025</strong></li>
        <li>Camera-ready Submission: <strong>11 August 2025</strong></li>
		<li>Conference dates: <strong>27 October 2025 - 31 October 2025</strong></li>
        </ul>
		Please note: The submission deadline is at 11:59 p.m. of the stated deadline date&nbsp;<a href="https://time.is/Anywhere_on_Earth"><font color="red"><u>Anywhere on Earth</u></font></a>.</p>

          </div>
        </div>
      </div>
    </section>

    <!--==========================================
    =            Papers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="submission">Submission</h2>
            </div>
            <li><strong>Submission Guidelines</strong>:</li>
			Submitted papers (.pdf format) must be the same format & template as the main conference. 
      The submition format The manuscript’s length is limited to one of the two options: a) 4 pages plus 1-page reference; or b) 8 pages plus up to 2-page reference.
      All papers will be peer-reviewed by experts in the field. Acceptance will be based on relevance to the workshop, scientific novelty, and technical quality. 
      <!--The workshop papers will be published in the ACM Digital Library.--> 
			<li><strong>Submission Site</strong>: <a href="https://easychair.org/my/conference?conf=lgm3a2025"><font color=#5672f9><u>Submission Link</u></font></a></li>
          </div>
        </div>
      </div>
    </section>	



	
    <!--==========================================
    =            Organizers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="organizers">Organizers</h2>
            </div>
			<ul style="margin-left: 40px">
	<li><a href="https://zhengwang125.github.io/"><font color=#5672f9><u>Zheng Wang</u></font></a> (Huawei Singapore Research Center, Singapore)</li>
	<li><font color=#5672f9><u>Qianqian Chen</u></font> (Huawei Singapore Research Center, Singapore)</li>
    <li><font color=#5672f9><u>Yiyang Luo</u></font> (Huawei Singapore Research Center, Singapore)</li>
    <li><font color=#5672f9><u>Zhiqiu Ye</u></font> (Huawei Singapore Research Center, Singapore)</li>
    <li><font color=#5672f9><u>Wei Shi</u></font> (Huawei Singapore Research Center, Singapore)</li>
	<li><a href="https://personal.ntu.edu.sg/hanwangzhang/"><font color=#5672f9><u>Hanwang Zhang</u></font></a> (Nanyang Technological University, Singapore)</li>
    <li><a href="https://www.chuatatseng.com/"><font color=#5672f9><u>Tat-Seng Chua</u></font></a> (National University of Singapore, Singapore)</li>
			</ul>

    <!--==========================================
    =            Speakers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="speakers">Speakers</h2>
            </div>

            <!-- ================= Keynote 1 ================= -->
            <div class="speaker-block">
              <h3><b>Keynote 1</b></h3>
              <img src="https://via.placeholder.com/170x170?text=Guosheng+Lin" class="media-left pull-left speaker-photo" width="170" alt="Guosheng Lin">
              <p><b>Speaker:</b> <a href="https://personal.ntu.edu.sg/guoshenglin/" target="_blank"><font color=#5672f9><u>Guosheng Lin</u></font></a> (Nanyang Technological University)</p>
              <p><b>Talk Title:</b> Recent Advances in 3D Generation: From 3D Assets to CAD Models</p>
              <p><b>Abstract:</b> In this talk, I will present our latest progress in 3D generative learning guided by text or image input. I will begin with our method for high-quality 3D asset generation from images, where we propose an efficient coarse-to-fine framework that combines compact coarse-level representations and part-aware voxel refinement. I will then introduce our approach for generating parametric CAD models directly from real-world images, eliminating the need for expensive 3D scanning. Together, these works push the frontier of visual generation and bring us closer to practical, scalable 3D modelling for real applications.</p>
              <p><b>Bio:</b> Guosheng Lin is an Associate Professor at the College of Computing and Data Science, Nanyang Technological University. His research interests are in computer vision, with a focus on data-efficient learning and generative learning. He has published over 100 research articles in prestigious venues. He serves as an Associate Editor for the IEEE journals TMM and TCSVT. He also serves as an Area Chair or Senior PC member for flagship conferences such as CVPR, ACM MM, IJCAI, and AAAI.</p>
            </div>

            <br/>

            <!-- ================= Keynote 2 ================= -->
            <div class="speaker-block">
              <h3><b>Keynote 2</b></h3>
              <img src="https://liuziwei7.github.io/homepage_files/me.png" class="media-left pull-left speaker-photo" width="170" alt="Ziwei Liu">
              <p><b>Speaker:</b> <a href="https://liuziwei7.github.io/" target="_blank"><font color=#5672f9><u>Ziwei Liu</u></font></a> (Nanyang Technological University)</p>
              <p><b>Talk Title:</b> From Multimodal Generative Models to Dynamic World Modeling</p>
              <p><b>Abstract:</b> Beyond the confines of flat screens, multimodal generative models are crucial to create immersive experiences in virtual reality, not only for human users but also for robotics. Virtual environments or real-world simulators, often comprised of complex 3D/4D assets, significantly benefit from the accelerated creation enabled by Gen AI. In this talk, we will introduce our latest research progress on multimodal generative models for objects, avatars, scenes, motions, and ultimately dynamic world models.</p>
              <p><b>Short Bio:</b> Ziwei Liu is currently an Associate Professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning and computer graphics. He has published extensively on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurIPS, ICLR, SIGGRAPH, TPAMI, TOG and Nature Machine Intelligence. He is the recipient of PAMI Mark Everingham Prize, CVPR Best Paper Award Candidate, Asian Young Scientist Fellowship, International Congress of Basic Science Frontiers of Science Award and MIT Technology Review Innovators under 35 Asia Pacific. He serves as an Area Chair of CVPR, ICCV, ECCV, NeurIPS and ICLR, as well as an Associate Editor of IJCV.</p>
            </div>

            <br/>

            <!-- ================= Keynote 3 ================= -->
            <div class="speaker-block">
              <h3><b>Keynote 3</b></h3>
              <img src="https://cde.nus.edu.sg/ece/wp-content/uploads/sites/3/2021/05/zhengshou_stanford_photo.jpg" class="media-left pull-left speaker-photo" width="170" height="180" alt="Mike Zheng Shou">
              <p><b>Speaker:</b> <a href="https://sites.google.com/view/showlab" target="_blank"><font color=#5672f9><u>Mike Zheng Shou</u></font></a> (National University of Singapore)</p>
              <p><b>Talk Title:</b> Video Intelligence in the Era of Multimodal</p>
              <!-- Abstract removed as requested -->
              <p><b>Bio:</b> Mike Shou is an Assistant Professor under Presidential Young Professorship at National University of Singapore. He was a Research Scientist at Facebook AI in the Bay Area. He obtained his Ph.D. degree at Columbia University with Prof Shih-Fu Chang. His research mainly focuses on video and multimodal. He received the Best Paper Finalist at CVPR 2022, Best Student Paper Nomination at CVPR 2017, EgoVis Distinguished Paper Award 2022/23. His team won 1st place in the international challenges including ActivityNet, EPIC-Kitchens, Ego4D. He is a ST Engineering Distinguished Professor and a Fellow of National Research Foundation Singapore. He is on the Forbes 30 Under 30 Asia list.</p>
            </div>

            <br/>

            <!-- =============== Keynote 4 (formerly Invited Talk) =============== -->
            <div class="speaker-block">
              <h3><b>Keynote 4</b></h3>
              <img src="https://via.placeholder.com/170x170?text=Hao+Fei" class="media-left pull-left speaker-photo" width="170" alt="Hao Fei">
              <p><b>Speaker:</b> <a href="http://haofei.vip/" target="_blank"><font color=#5672f9><u>Hao Fei</u></font></a> (National University of Singapore)</p>
              <p><b>Talk Title:</b> On Path to Multimodal Generalist: General-Level and General-Bench</p>
              <p><b>Abstract:</b> AI systems are increasingly capable of handling diverse types of data—such as text, images, and audio. However, many of these multimodal systems excel only in specific tasks or data modalities, lacking the broad adaptability seen in human intelligence. Also the existing evaluation paradigm that simply assumes that higher performance across tasks indicates a stronger MLLM capability can be problematic. This research introduces two tools/resources: General-Level, a framework that assesses an AI model's ability to integrate and apply knowledge across different tasks and data types; and General-Bench, a comprehensive dataset comprising over 700 tasks and 325,000 examples designed to evaluate this integrative capability. By applying these tools to over 100 existing AI models, we discovered that while some models perform well on individual tasks, they often struggle to transfer knowledge between different types of tasks or data. This indicates a gap in achieving truly general-purpose multimodal AGI. Our work aims to guide the development of more versatile multimodal AI systems that can seamlessly understand and generate multiple forms of data, moving us closer to AI that mirrors human-like general intelligence.</p>
              <p><b>Bio:</b> Hao Fei is a Senior Research Fellow at National University of Singapore. His research focuses on vision-language understanding and generation, multimodal large foundation models. He has published over 60 papers in top-tier venues such as IEEE TPAMI, IEEE TKDE, ACM TOIS, AI, ICML, NeurIPS, CVPR, ACL, ICLR, and AAAI, gaining over 7k citations. He has received many accolades including 2022 Outstanding CIPSC PhD Dissertation Award, 2023 WAIC Rising Star Award, 2024 WAIC Excellent Young Paper Award, and Stanford World's Top 2% Scientist. His representative contributions include NExT-GPT, Vitron, and the Generalist Benchmark. He has organized over 20 international workshops, tutorials, and shared tasks, and frequently serves as Area Chair/Senior PC for top conferences including CVPR, ICML, NeurIPS, AAAI, ACL, IJCAI, and ACM MM, as well as Associate Editor of ACM TALLIP and Neurocomputing.</p>
            </div>

          </div>
        </div>
      </div>
    </section>

    <!--===========================================
    =            Workshop Arrangement (UPDATED)   =
    ============================================-->
    <section class="popular-deals section bg-white">
      <div class="container">
        <div class="row">
          <div class="col-md-12">
            <div class="section-title">
              <h2 id="schedule">Workshop Schedule</h2>
            </div>
            <p align="center">
                <li>On-site venue: <b>TBD</b></li>
                <li>Date &amp; Time: <b>October 27th Morning (Dublin Time)</b></li>
                <li>Zoom link: <a><font color=#5672f9><u>Meeting ID: </u></font></a> &nbsp; <a><font color=#5672f9><u>Passcode: </u></font></a></li>
                <br/>
                <style type="text/css">
                  #myta td{
                    padding-right: 8px;
                    padding-left: 8px;
                    padding-top: 4px;
                    padding-bottom: 4px;
                  }
                </style>
                <table border="1" id="myta">
                  <tr> 
                    <td><strong>Time (Dublin)</strong></td> 
                    <td><strong>Session</strong></td>
                  </tr>
                  <tr> <td>09:00 – 09:10</td> <td>Welcome Message from the Chairs</td></tr>

                  <!-- Four talks first: 35min talk + 5min Q/A each -->
                  <tr> <td>09:10 – 09:50</td> <td>Keynote 1 — Recent Advances in 3D Generation: From 3D Assets to CAD Models (Guosheng Lin)</td></tr>
                  <tr> <td>09:50 – 10:30</td> <td>Keynote 2 — From Multimodal Generative Models to Dynamic World Modeling (Ziwei Liu)</td></tr>
                  <tr> <td>10:30 – 11:10</td> <td>Keynote 3 — Video Intelligence in the Era of Multimodal (Mike Zheng Shou)</td></tr>
                  <tr> <td>11:10 – 11:50</td> <td>Keynote 4 — On Path to Multimodal Generalist: General-Level and General-Bench (Hao Fei)</td></tr>

                  <!-- Papers after talks: 8min + 2min Q/A each -->
                  <tr> <td>11:50 – 12:00</td> <td>Paper Presentation 1 — Annotation-Free Prompt Expansion for Chinese Text-to-Image Generation</td></tr>
                  <tr> <td>12:00 – 12:10</td> <td>Paper Presentation 2 — Enabling Dynamic Storytelling via Training-Free Multimodal Synchronized Video Synthesis with Character Consistency</td></tr>
                </table>
              </p>
          </div>
        </div>
      </div>
    </section>


    
    <!--============================
    =            Footer            =
    =============================-->
    
    <!-- Footer Bottom -->
    <!-- footer class="footer-bottom"> 
      <!-- Container Start -->
      <div class="container">
        <div class="row">
          <div class="col-sm-12 col-12"> 
            <!-- Copyright -->
            <div class="copyright">
              <p class="text-center">
                For any questions, please email to <a href="mailto:LGM3A2024@gmail.com"><font color=#5672f9><u>LGM3A2024@gmail.com</u></font></a>
              </p>
                <br class="clear">
            </div>
          </div>
        </div>
      </div>
      <!-- Container End --> 
    </footer>
    
    <!-- JAVASCRIPTS --> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery-ui.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/tether.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.raty-fa.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/popper.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/bootstrap.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/bootstrap-slider.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/slick.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.nice-select.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.fancybox.pack.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/SmoothScroll.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/scripts.js.download"></script -->
    
    
    

</body></html>
